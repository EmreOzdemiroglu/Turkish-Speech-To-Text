{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSAHCP6BGRRLTPw+dlIiqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmreOzdemiroglu/Turkish-Speech-To-Text/blob/main/Transcription_Diarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Change runtime to GPU. Runtime / Change runtime type\n",
        "- The transcript will be saved in Files, click on the folder icon on the left menu.\n"
      ],
      "metadata": {
        "id": "IlYnUFZ1FmmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
        "!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n",
        "!pip install torch numpy scikit-learn wave\n",
        "!apt-get install -y ffmpeg"
      ],
      "metadata": {
        "id": "LXCPgdsnJz4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "path = next(iter(uploaded))"
      ],
      "metadata": {
        "id": "OTXhI4QKQVAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import whisper\n",
        "import datetime\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "import wave\n",
        "import contextlib\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CJwWUm8mRUzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZvipjiYFj_T"
      },
      "outputs": [],
      "source": [
        "def convert_to_wav(file_path):\n",
        "    \"\"\"Convert file to WAV format if not already in that format.\"\"\"\n",
        "    if file_path[-3:] != 'wav':\n",
        "        subprocess.call(['ffmpeg', '-i', file_path, 'audio.wav', '-y'])\n",
        "        return 'audio.wav'\n",
        "    return file_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_embedding_model():\n",
        "    \"\"\"Load the speaker embedding model.\"\"\"\n",
        "    return PretrainedSpeakerEmbedding(\n",
        "        \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "        device=torch.device(\"cuda\")\n",
        "    )\n",
        "\n",
        "def get_file_duration(file_path):\n",
        "    \"\"\"Return the duration of the audio file.\"\"\"\n",
        "    with contextlib.closing(wave.open(file_path, 'r')) as f:\n",
        "        frames = f.getnframes()\n",
        "        rate = f.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "    return duration\n",
        "\n",
        "def segment_embedding(audio_path, segment, duration, embedding_model):\n",
        "    \"\"\"Get the embedding for a given audio segment.\"\"\"\n",
        "    start = segment[\"start\"]\n",
        "    end = min(duration, segment[\"end\"])\n",
        "    clip = Segment(start, end)\n",
        "    waveform, _ = Audio().crop(audio_path, clip)\n",
        "    return embedding_model(waveform[None])\n",
        "\n",
        "def get_embeddings(audio_path, segments, duration, embedding_model):\n",
        "    \"\"\"Generate embeddings for each segment.\"\"\"\n",
        "    embeddings = np.zeros(shape=(len(segments), 192))\n",
        "    for i, segment in enumerate(segments):\n",
        "        embeddings[i] = segment_embedding(audio_path, segment, duration, embedding_model)\n",
        "    return np.nan_to_num(embeddings)\n"
      ],
      "metadata": {
        "id": "UVb3-O0oJYzR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def write_transcript(segments):\n",
        "    \"\"\"Write the transcript to a file.\"\"\"\n",
        "    with open(\"transcript.txt\", \"w\") as f:\n",
        "        for (i, segment) in enumerate(segments):\n",
        "            if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "                f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(datetime.timedelta(seconds=round(segment[\"start\"]))) + '\\n')\n",
        "            f.write(segment[\"text\"][1:] + ' ')\n",
        "\n",
        "def transcribe_audio(path: str):\n",
        "    \"\"\"\n",
        "    Transcribe the audio file and return the transcript.\n",
        "\n",
        "    Args:\n",
        "    - path (str): Path to the audio file.\n",
        "\n",
        "    Returns:\n",
        "    - str: Path to the transcript file.\n",
        "    \"\"\"\n",
        "    # Convert the audio file to WAV format if required\n",
        "    path = convert_to_wav(path)\n",
        "\n",
        "    # Load the Whisper model and transcribe the audio\n",
        "    model = whisper.load_model('large')\n",
        "    result = model.transcribe(path)\n",
        "    segments = result[\"segments\"]\n",
        "\n",
        "    # Get the duration of the audio file and load the embedding model\n",
        "    duration = get_file_duration(path)\n",
        "    embedding_model = load_embedding_model()\n",
        "    embeddings = get_embeddings(path, segments, duration, embedding_model)\n",
        "\n",
        "    # Perform clustering to identify speakers\n",
        "    clustering = AgglomerativeClustering(2).fit(embeddings)\n",
        "    labels = clustering.labels_\n",
        "    for i in range(len(segments)):\n",
        "        segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "    # Write the transcript to a file\n",
        "    transcript_file = path.rsplit('.', 1)[0] + '_transcription.txt'\n",
        "    with open(transcript_file, \"w\") as f:\n",
        "        for (i, segment) in enumerate(segments):\n",
        "            if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "                f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(datetime.timedelta(seconds=round(segment[\"start\"]))) + '\\n')\n",
        "            f.write(segment[\"text\"][1:] + ' ')\n",
        "\n",
        "    return transcript_file"
      ],
      "metadata": {
        "id": "TQkzmsztIgzZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcribe_audio(path)"
      ],
      "metadata": {
        "id": "udE0_HHcFzbY"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}